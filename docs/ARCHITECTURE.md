# Arquitetura do Sistema - Books to Scrape API

## VisÃ£o Geral

O sistema implementa um pipeline completo de ETL (Extract, Transform, Load) para coleta, processamento e disponibilizaÃ§Ã£o de dados de livros atravÃ©s de uma API RESTful.

## Diagrama de Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PIPELINE DE DADOS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EXTRAÃ‡ÃƒO (Web Scraping)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  books.toscrape.com  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Scraper Python     â”‚
   â”‚   - BeautifulSoup    â”‚
   â”‚   - Requests         â”‚
   â”‚   - Retry Logic      â”‚
   â”‚   - Rate Limiting    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   data/books.csv     â”‚
   â”‚   (1000+ registros)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. TRANSFORMAÃ‡ÃƒO & SERVIR
              â”‚
              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   API FastAPI        â”‚
   â”‚   - Endpoints REST   â”‚
   â”‚   - PaginaÃ§Ã£o        â”‚
   â”‚   - Filtros          â”‚
   â”‚   - Busca            â”‚
   â”‚   - Features ML      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Redis Cache        â”‚
   â”‚   (opcional)         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. CONSUMO
              â”‚
              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Clientes (Apps, Data Science, ML)      â”‚
   â”‚  - Web Applications                      â”‚
   â”‚  - Jupyter Notebooks                     â”‚
   â”‚  - ML Training Pipelines                 â”‚
   â”‚  - Analytics Dashboards                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Componentes

### 1. Web Scraper (`scripts/scraper.py`)

**Responsabilidades:**
- Extrair dados estruturados de https://books.toscrape.com/
- Tratamento robusto de erros e retry logic
- Rate limiting para nÃ£o sobrecarregar o servidor
- Logging detalhado de todas as operaÃ§Ãµes

**CaracterÃ­sticas:**
- **ResiliÃªncia**: Retry automÃ¡tico com backoff exponencial
- **Rate Limiting**: Delay configurÃ¡vel entre requisiÃ§Ãµes
- **Logging**: Logs detalhados em `logs/scraper.log`
- **Incremental**: Pode ser executado mÃºltiplas vezes
- **ValidaÃ§Ã£o**: Valida dados antes de salvar

**Esquema de Dados ExtraÃ­dos:**
```python
{
    'id': int,                      # ID Ãºnico
    'title': str,                   # TÃ­tulo do livro
    'price': float,                 # PreÃ§o em Â£
    'availability': str,            # Status de disponibilidade
    'availability_copies': int,     # NÃºmero de cÃ³pias
    'rating': int,                  # Rating 1-5
    'category': str,                # Categoria/gÃªnero
    'product_page_url': str,        # URL da pÃ¡gina
    'upc': str,                     # CÃ³digo UPC
    'description': str,             # DescriÃ§Ã£o
    'image_url': str,               # URL da imagem
    'scraped_at': str              # Timestamp ISO
}
```

### 2. API RESTful (`api/`)

**Stack TÃ©cnica:**
- **Framework**: FastAPI 0.104+
- **Server**: Uvicorn (ASGI)
- **ValidaÃ§Ã£o**: Pydantic models
- **DocumentaÃ§Ã£o**: OpenAPI/Swagger automÃ¡tica

**Endpoints Principais:**

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| GET | `/` | InformaÃ§Ãµes da API |
| GET | `/health` | Health check |
| GET | `/books` | Lista paginada com filtros |
| GET | `/books/{id}` | Detalhes de um livro |
| GET | `/books/search` | Busca por termo |
| GET | `/books/genres` | Lista de categorias |
| GET | `/books/genre/{genre}` | Livros por categoria |
| GET | `/stats` | EstatÃ­sticas agregadas |
| GET | `/ml/sample` | Amostra para ML |

**Features:**
- âœ… PaginaÃ§Ã£o completa
- âœ… Filtros (preÃ§o, rating, categoria)
- âœ… OrdenaÃ§Ã£o (ASC/DESC)
- âœ… Busca full-text
- âœ… CORS habilitado
- âœ… DocumentaÃ§Ã£o Swagger
- âœ… Features para ML
- âœ… EstatÃ­sticas agregadas

### 3. Camada de Cache (Opcional)

**Redis**:
- Cache de listagens frequentes
- TTL configurÃ¡vel
- InvalidaÃ§Ã£o automÃ¡tica

## EstratÃ©gia de Escalabilidade

### Horizontal Scaling

1. **Load Balancer**: Nginx/HAProxy na frente da API
2. **MÃºltiplas InstÃ¢ncias**: Deploy em Kubernetes/Docker Swarm
3. **Auto-scaling**: Baseado em mÃ©tricas de CPU/memÃ³ria

### Data Layer

1. **Database Migration**:
   - Migrar de CSV para PostgreSQL/MongoDB
   - Ãndices em campos de busca (title, category)
   - Full-text search com PostgreSQL

2. **Cache Layer**:
   - Redis para queries frequentes
   - TTL de 5-15 minutos
   - Cache warming em horÃ¡rios de baixo trÃ¡fego

3. **CDN**:
   - CloudFlare/AWS CloudFront para API responses
   - Cache de imagens dos livros

### Processamento AssÃ­ncrono

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraper   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Message     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Workers    â”‚
â”‚   Trigger   â”‚         â”‚  Queue       â”‚         â”‚  (Celery)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  (RabbitMQ)  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Database    â”‚
                        â”‚  (Postgres)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Monitoramento

- **APM**: DataDog, New Relic
- **Logs**: ELK Stack (Elasticsearch, Logstash, Kibana)
- **Metrics**: Prometheus + Grafana
- **Alertas**: PagerDuty

## Casos de Uso para Data Science

### 1. AnÃ¡lise ExploratÃ³ria

```python
import requests
import pandas as pd

# Obter dados
response = requests.get('http://api.example.com/books?per_page=1000')
df = pd.DataFrame(response.json()['books'])

# AnÃ¡lises
print(df['price'].describe())
print(df['category'].value_counts())
```

### 2. Feature Engineering

A API jÃ¡ fornece features prontas:
- `price_normalized`: PreÃ§o normalizado (0-1)
- `rating_normalized`: Rating normalizado (0-1)
- `price_category`: Budget, Moderate, Premium, Luxury
- `has_description`: Boolean

### 3. Datasets para Treinamento

```python
# Amostra reproduzÃ­vel para ML
response = requests.get('http://api.example.com/ml/sample?size=500&random_state=42')
train_data = response.json()['data']
```

## IntegraÃ§Ã£o com ML Pipelines

### Fluxo de Trabalho

```
1. COLETA DE DADOS
   â””â”€â–¶ GET /ml/sample?size=1000

2. PRÃ‰-PROCESSAMENTO
   â””â”€â–¶ Features jÃ¡ normalizadas
   â””â”€â–¶ Encoding de categorias
   â””â”€â–¶ Split train/test

3. TREINAMENTO
   â””â”€â–¶ Modelos: RegressÃ£o de preÃ§o, ClassificaÃ§Ã£o de rating

4. PREDIÃ‡ÃƒO
   â””â”€â–¶ Novo endpoint: POST /ml/predict

5. FEEDBACK LOOP
   â””â”€â–¶ Armazenar prediÃ§Ãµes
   â””â”€â–¶ Re-treinar periodicamente
```

### Endpoints Futuros para ML

```python
# Proposta de endpoints adicionais
POST /ml/predict          # PrediÃ§Ã£o de preÃ§o/rating
GET  /ml/models           # Listar modelos disponÃ­veis
GET  /ml/model/{id}       # Detalhes de um modelo
POST /ml/train            # Trigger treinamento
GET  /ml/metrics          # MÃ©tricas dos modelos
```

### Versionamento de Datasets

```bash
# Estrutura proposta
data/
â”œâ”€â”€ v1.0/
â”‚   â”œâ”€â”€ books_20240101.csv
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ v1.1/
â”‚   â”œâ”€â”€ books_20240201.csv
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ latest -> v1.1/
```

## SeguranÃ§a

### Implementado
- âœ… CORS configurado
- âœ… Rate limiting (via scraper)
- âœ… ValidaÃ§Ã£o de inputs (Pydantic)
- âœ… Health checks

### RecomendaÃ§Ãµes Futuras
- ğŸ”„ AutenticaÃ§Ã£o (API Keys, OAuth2)
- ğŸ”„ Rate limiting na API (limitaÃ§Ãµes por IP)
- ğŸ”„ HTTPS obrigatÃ³rio
- ğŸ”„ Input sanitization avanÃ§ado
- ğŸ”„ Audit logging

## Performance

### OtimizaÃ§Ãµes Atuais
- Carregamento de dados em memÃ³ria (DataFrame)
- Filtros e buscas otimizados com Pandas
- PaginaÃ§Ã£o para evitar payloads grandes

### Benchmarks Esperados
- LatÃªncia: < 100ms (p95)
- Throughput: ~1000 req/s (single instance)
- Memory: ~200MB (com 1000 livros)

### OtimizaÃ§Ãµes Futuras
```python
# Ãndices de banco de dados
CREATE INDEX idx_category ON books(category);
CREATE INDEX idx_price ON books(price);
CREATE INDEX idx_rating ON books(rating);
CREATE INDEX idx_title_fulltext ON books USING GIN(to_tsvector('english', title));

# Caching
@cache(expire=300)
def get_books_list(...):
    ...
```

## ManutenÃ§Ã£o

### Backups
- Backup diÃ¡rio do CSV
- Versionamento no Git
- Arquivos de log rotacionados

### Monitoramento
- Health check endpoint
- Logs estruturados
- MÃ©tricas de uso

### AtualizaÃ§Ãµes
```bash
# Re-executar scraper periodicamente
0 2 * * * cd /app && python scripts/scraper.py
```

## Tecnologias

| Componente | Tecnologia | VersÃ£o |
|------------|-----------|--------|
| Linguagem | Python | 3.11+ |
| Web Framework | FastAPI | 0.104+ |
| Web Server | Uvicorn | 0.24+ |
| Scraping | BeautifulSoup4 | 4.12+ |
| HTTP Client | Requests | 2.31+ |
| Data Processing | Pandas | 2.1+ |
| Validation | Pydantic | 2.5+ |
| Testing | Pytest | 7.4+ |
| Cache | Redis | 7+ |
| Container | Docker | 24+ |
| CI/CD | GitHub Actions | - |
| Deploy | Render/Heroku | - |

## ConclusÃ£o

Esta arquitetura fornece:
- âœ… Pipeline completo de dados
- âœ… API escalÃ¡vel e documentada
- âœ… Pronto para integraÃ§Ã£o com ML
- âœ… Monitoramento e observabilidade
- âœ… Deploy automatizado
- âœ… Testes automatizados

